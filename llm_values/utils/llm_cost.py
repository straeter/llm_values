import tiktoken

encoding = tiktoken.get_encoding("cl100k_base")

llm_prices = {
    "gpt-4o-2024-05-13": {
        "input": 5 / 1000,
        "output": 15 / 1000,
    },
    "gpt-3.5-turbo-0125": {
        "input": 0.50 / 1000,
        "output": 1.50 / 1000,
    },
    "mistral-large-latest": {
        "input": 3.8 / 1000,
        "output": 11.3 / 1000,
    },
    "mistral-small-latest": {
        "input": 0.9 / 1000,
        "output": 2.8 / 1000,
    },
    "claude-3-opus-20240229": {
        "input": 15 / 1000,
        "output": 75 / 1000
    },
    "claude-3-sonnet-20240229": {
        "input": 3 / 1000,
        "output": 15 / 1000
    },
    "claude-3-haiku-20240307": {
        "input": 0.25 / 1000,
        "output": 1.25 / 1000
    }
}


def confirm_to_continue(message):
    while True:
        try:
            response = input(f"{message}\nDo you wish to continue? (yes/y or no/n): ").strip().lower()
            if response in ('yes', 'y'):
                print("Continuing...")
                break
            elif response in ('no', 'n'):
                print("Aborting...")
                quit()
            else:
                print("Invalid input. Please answer with 'yes/y' or 'no/n'.")
        except KeyboardInterrupt:
            print("\nOperation cancelled by user.")
            break

# def estimate_tokens(text, factor=1.4):
#     words = len(text.split())  # Split text by whitespace and count the elements
#     estimated_tokens = int(words * factor)
#
#     return estimated_tokens

def estimate_tokens(text):
    tokens = encoding.encode(text)

    return len(tokens)


def estimate_cost(items, multiplier=1, output_multiplier=1, max_token: int = None, model="gpt-4o-2024-05-13",
                  budget=0.1):
    """Estimate the cost of a LLM query
    :param items: A list of strings (inputs)
    :param multiplier: Multiplier to total cost (e.g. number of languages to translate to)
    :param max_token: (Alternative) Maximum number of tokens to use for output
    :param output_multiplier: How much longer the output (response) is compared to the input (prompt)
    :param model: llm model to query
    :param budget: budget for you llm calls (get a warning if exceeded)
    """

    whole_string = " ".join(items)
    token_count = estimate_tokens(whole_string)
    cost_input = token_count * llm_prices[model]["input"] / 1000
    if max_token:
        cost_output = max_token * llm_prices[model]["output"] / 1000
    else:
        cost_output = token_count * output_multiplier * llm_prices[model]["output"] / 1000
    cost = (cost_input + cost_output) * multiplier
    print(f"Estimated cost: ${cost:.2f}")
    if cost > budget:
        warning_message = f"This LLM query will cost approximately ${cost:.2f}."
        confirm_to_continue(warning_message)
